{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\PC-\n",
      "[nltk_data]     MIKE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\PC-\n",
      "[nltk_data]     MIKE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw to C:\\Users\\PC-\n",
      "[nltk_data]     MIKE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\omw.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\PC-\n",
      "[nltk_data]     MIKE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', '...', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentence= \"At eight o'clock on Thursday morning... Arthur didn't feel very good.\"\n",
    "tokens = nltk.word_tokenize(sentence, language='english')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola', 'buen', 'día', 'saludos', 'desde', 'ecuador', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sentence= \"Hola buen día saludos desde ecuador.\"\n",
    "tokens = nltk.word_tokenize(sentence, language='spanish')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'alost', 'dead', 'this', 'time']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "sent_ = \"I am alost dead this time\"\n",
    "tokens = nltk.word_tokenize(sent_)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('mirrorlike.s.01')]\n",
      "capable of reflecting light like a mirror\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "word_ = wordnet.synsets(\"specular\")\n",
    "print(word_)\n",
    "print(word_[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('spectacular.n.01'), Synset('dramatic.s.02'), Synset('spectacular.s.02'), Synset('outstanding.s.02')]\n",
      "a lavishly produced performance\n",
      "sensational in appearance or thrilling in effect\n",
      "characteristic of spectacles or drama\n",
      "having a quality that thrusts itself into attention\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "word_ = wordnet.synsets(\"spectacular\")\n",
    "print(word_)\n",
    "print(word_[0].definition())\n",
    "print(word_[1].definition())\n",
    "print(word_[2].definition())\n",
    "print(word_[3].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('hello.n.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "word_ = wordnet.synsets(\"hola\", pos=None, lang='spa')\n",
    "print(word_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('diggings.n.02'), Synset('dwelling.n.01'), Synset('house.n.01'), Synset('house.n.06'), Synset('firm.n.01'), Synset('manufacturer.n.01'), Synset('family.n.01'), Synset('house.n.09'), Synset('home.n.03'), Synset('home.n.01'), Synset('home.n.07')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "word_ = wordnet.synsets(\"casa\", pos=None, lang='spa')\n",
    "print(word_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('cannabis.n.02'), Synset('hashish.n.01'), Synset('pot.n.09'), Synset('chocolate.n.02'), Synset('cocoa.n.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "word_ = wordnet.synsets(\"chocolate\", pos=None, lang='spa')\n",
    "print(word_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('pastry.n.01'), Synset('pie_crust.n.01'), Synset('pasta.n.02'), Synset('spread.n.05'), Synset('dough.n.01'), Synset('boodle.n.01'), Synset('paste.n.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "word_ = wordnet.synsets(\"pasta\", pos=None, lang='spa')\n",
    "print(word_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('cream.n.01'), Synset('cream.n.02'), Synset('cream.n.03'), Synset('cream.v.01'), Synset('cream.v.02'), Synset('cream.v.03'), Synset('skim.v.06'), Synset('cream.v.05')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "word_ = wordnet.synsets(\"cream\")\n",
    "print(word_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decreas\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem(\"decreases\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decrease\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() # Create the Lemmatizer\n",
    "object\n",
    "print(lemmatizer.lemmatize(\"decreases\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Regular Expresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['very', 'nice', 'lecture', 'last']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "words = ['very','nice','lecture','day','moon','last']\n",
    "expression = '|'.join(words)\n",
    "re.findall(expression, 'i attended a very nice lecture last year', re.M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Are', 'you', 'sure', 'moving', 'ahead', 'on', 'this', 'route', 'is', 'the', 'right', 'thing?']\n",
      "['Are you sure moving ahead on this route is the right thing?\\n']\n"
     ]
    }
   ],
   "source": [
    "text_file = 'data.txt'\n",
    "# Method-1 : Individual words as separate elements of the list\n",
    "with open(text_file) as f:\n",
    "    words = f.read().split()\n",
    "print(words)\n",
    "# Method-2 : Whole text as single element of the list\n",
    "f = open(text_file , 'r')\n",
    "words_ = f.readlines()\n",
    "print(words_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['john', 'has', 'been', 'selected', 'for', 'the', 'trial', 'phase', 'this', 'time.', 'congrats']\n",
      "{'selected', 'trial', 'phase', 'been', 'john', 'time.', 'for', 'the', 'this', 'has'}\n"
     ]
    }
   ],
   "source": [
    "sentence = 'John has been selected for the trial phase this time. Congrats!!'\n",
    "sentence=sentence.lower()\n",
    "# defining the positive and negative words explicitly\n",
    "positive_words=['awesome','good', 'nice', 'super', 'fun', 'delightful','congrats']\n",
    "negative_words=['awful','lame','horrible','bad']\n",
    "sentence=sentence.replace('!','')\n",
    "words= sentence.split(' ')\n",
    "print(words)\n",
    "result= set(words)-set(positive_words)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'about', 'Deep', 'Learning', 'and', 'Natural', 'Language', 'Processing', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "sentence= \"This book is about Deep Learning and Natural Language Processing!\"\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'book', 'Deep', 'Learning', 'Natural', 'Language', 'Processing', '!']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "new_tokens = [w for w in tokens if not w in stop_words]\n",
    "new_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traducir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola papa estas enojado.  ->  Hello dad, you're angry.\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "translation = translator.translate('Hola papa estas enojado.', dest='en')\n",
    "print(translation.origin, ' -> ', translation.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, Dad, you're angry.\n"
     ]
    }
   ],
   "source": [
    "from translate import Translator\n",
    "translator= Translator(from_lang=\"spanish\",to_lang=\"english\")\n",
    "translation = translator.translate(\"Hola papa estas enojado.\")\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counter Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['also', 'and', 'classic', 'classical', 'he', 'listens', 'music', 'old', 'pop', 'ramiess', 'rock', 'sings', 'songs', 'to']\n",
      "[[0 0 1 0 0 0 0 0 0 1 0 1 1 0]\n",
      " [0 0 0 0 1 1 0 1 1 0 0 0 0 1]\n",
      " [0 1 0 0 0 0 1 0 0 0 1 0 0 0]\n",
      " [1 1 0 1 0 1 0 0 0 0 0 0 1 1]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "texts=[\"Ramiess sings classic songs\",\"he listens to old pop \",\n",
    "\"and rock music\", ' and also listens to classical songs','and to to to']\n",
    "cv = CountVectorizer()\n",
    "cv_fit=cv.fit_transform(texts)\n",
    "print(cv.get_feature_names())\n",
    "print(cv_fit.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['also', 'and', 'classic', 'classical', 'he', 'listens', 'music', 'old', 'pop', 'ramiess', 'rock', 'sings', 'songs', 'to']\n",
      "  (0, 12)\t0.41428875116588965\n",
      "  (0, 2)\t0.5254727492640658\n",
      "  (0, 11)\t0.5254727492640658\n",
      "  (0, 9)\t0.5254727492640658\n",
      "  (1, 8)\t0.4854606118156975\n",
      "  (1, 7)\t0.4854606118156975\n",
      "  (1, 13)\t0.3827427224171519\n",
      "  (1, 5)\t0.3827427224171519\n",
      "  (1, 4)\t0.4854606118156975\n",
      "  (2, 6)\t0.6176143709756019\n",
      "  (2, 10)\t0.6176143709756019\n",
      "  (2, 1)\t0.48693426407352264\n",
      "  (3, 3)\t0.47212002654617047\n",
      "  (3, 0)\t0.47212002654617047\n",
      "  (3, 1)\t0.3722248517590162\n",
      "  (3, 13)\t0.3722248517590162\n",
      "  (3, 5)\t0.3722248517590162\n",
      "  (3, 12)\t0.3722248517590162\n",
      "[[0.         0.         0.52547275 0.         0.         0.\n",
      "  0.         0.         0.         0.52547275 0.         0.52547275\n",
      "  0.41428875 0.        ]\n",
      " [0.         0.         0.         0.         0.48546061 0.38274272\n",
      "  0.         0.48546061 0.48546061 0.         0.         0.\n",
      "  0.         0.38274272]\n",
      " [0.         0.48693426 0.         0.         0.         0.\n",
      "  0.61761437 0.         0.         0.         0.61761437 0.\n",
      "  0.         0.        ]\n",
      " [0.47212003 0.37222485 0.         0.47212003 0.         0.37222485\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.37222485 0.37222485]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "texts=[\"Ramiess sings classic songs\",\"he listens to old pop\",\n",
    "\"and rock music\", ' and also listens to classical songs']\n",
    "vect = TfidfVectorizer()\n",
    "X = vect.fit_transform(texts)\n",
    "print(vect.get_feature_names())\n",
    "print(X)\n",
    "print(X.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "data = [\n",
    "('I love my country.', 'pos'),\n",
    "('This is an amazing place!', 'pos'),\n",
    "('I do not like the smell of this place.', 'neg'),\n",
    "('I do not like this restaurant', 'neg'),\n",
    "('I am tired of hearing your nonsense.', 'neg'),\n",
    "(\"I always aspire to be like him\", 'pos'),\n",
    "(\"It's a horrible performance.\", \"neg\")\n",
    "]\n",
    "model = NaiveBayesClassifier(data)\n",
    "model.classify(\"It's an awesome place!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
